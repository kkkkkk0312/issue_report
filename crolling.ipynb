{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfe628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 설정\n",
    "BASE_URL = 'https://m.dcinside.com'\n",
    "GALLERY_ID = 'programming'  # 예시: 프로그래밍 갤러리 (네가 원하는 걸로 바꿔)\n",
    "MAX_POSTS = 100\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "\n",
    "# 결과 저장\n",
    "posts = []\n",
    "\n",
    "# Step 1: 인기글 리스트 가져오기\n",
    "def get_popular_posts(gallery_id):\n",
    "    url = f'{BASE_URL}/board/{gallery_id}?recommend=1'  # 개념글 목록\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    links = soup.select('tr.ub-content > td.gall_tit > a')\n",
    "    post_links = [BASE_URL + link['href'] for link in links]\n",
    "    return post_links\n",
    "\n",
    "# Step 2: 게시글 본문 + 댓글 가져오기\n",
    "def get_post_details(post_url):\n",
    "    res = requests.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one('div.ub-content > div.title_subject').text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    try:\n",
    "        body = soup.select_one('div.write_div').text.strip()\n",
    "    except:\n",
    "        body = \"\"\n",
    "\n",
    "    try:\n",
    "        like_count = int(soup.select_one('span.up_num').text.strip())\n",
    "    except:\n",
    "        like_count = 0\n",
    "\n",
    "    try:\n",
    "        comment_count = int(soup.select_one('span.cmt_count').text.strip().replace('댓글', '').strip())\n",
    "    except:\n",
    "        comment_count = 0\n",
    "\n",
    "    try:\n",
    "        time_tag = soup.select_one('div.gall_date')['title']\n",
    "    except:\n",
    "        time_tag = \"\"\n",
    "\n",
    "    post_id = post_url.split('/')[-1]\n",
    "    \n",
    "    comments = get_comments(post_id)\n",
    "    \n",
    "    return {\n",
    "        'post_id': post_id,\n",
    "        'title': title,\n",
    "        'body': body,\n",
    "        'like_count': like_count,\n",
    "        'comment_count': comment_count,\n",
    "        'time': time_tag,\n",
    "        'link': post_url,\n",
    "        'comments': comments\n",
    "    }\n",
    "\n",
    "# Step 3: 댓글 가져오기\n",
    "def get_comments(post_id):\n",
    "    comments = []\n",
    "    page = 1\n",
    "\n",
    "    while len(comments) < MAX_COMMENTS_PER_POST:\n",
    "        comment_url = f\"https://m.dcinside.com/board/comment/{post_id}?page={page}\"\n",
    "        res = requests.get(comment_url)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        comment_list = soup.select('ul.comment_box > li')\n",
    "        \n",
    "        if not comment_list:\n",
    "            break  # 더 이상 댓글 없음\n",
    "        \n",
    "        for comment in comment_list:\n",
    "            content_tag = comment.select_one('p.usertxt')\n",
    "            if content_tag:\n",
    "                content = content_tag.text.strip()\n",
    "                comments.append(content)\n",
    "            \n",
    "            if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.1)  # 서버 부하 줄이기\n",
    "\n",
    "    return comments\n",
    "\n",
    "# 메인 크롤링\n",
    "def crawl_dc_popular(gallery_id):\n",
    "    post_links = get_popular_posts(gallery_id)\n",
    "    print(f\"Found {len(post_links)} popular posts.\")\n",
    "    \n",
    "    for i, link in enumerate(post_links[:MAX_POSTS]):\n",
    "        print(f\"[{i+1}/{MAX_POSTS}] 크롤링 중: {link}\")\n",
    "        post = get_post_details(link)\n",
    "        posts.append(post)\n",
    "        time.sleep(0.5)  # 서버 부하 줄이기\n",
    "\n",
    "    # CSV 저장\n",
    "    df = pd.DataFrame(posts)\n",
    "    df.to_csv(f'dc_{gallery_id}_popular.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"CSV 저장 완료\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_dc_popular(GALLERY_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f76a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 popular posts.\n",
      "CSV 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 설정\n",
    "BASE_URL = 'https://gall.dcinside.com'\n",
    "MAIN_POPULAR_URL = f'{BASE_URL}/board/lists?id=all&exception_mode=recommend'\n",
    "MAX_POSTS = 100\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "\n",
    "# 결과 저장\n",
    "posts = []\n",
    "\n",
    "# Step 1: 메인 인기글 목록 가져오기\n",
    "def get_main_popular_posts():\n",
    "    res = requests.get(MAIN_POPULAR_URL)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    links = soup.select('tr.ub-content > td.gall_tit > a')\n",
    "    post_links = [BASE_URL + link['href'] for link in links if '/board/view/' in link['href']]\n",
    "    return post_links\n",
    "\n",
    "# Step 2: 게시글 본문 + 댓글 가져오기\n",
    "def get_post_details(post_url):\n",
    "    res = requests.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one('span.title_subject').text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    try:\n",
    "        body = soup.select_one('div.write_div').text.strip()\n",
    "    except:\n",
    "        body = \"\"\n",
    "\n",
    "    try:\n",
    "        like_count = int(soup.select_one('span.up_num').text.strip())\n",
    "    except:\n",
    "        like_count = 0\n",
    "\n",
    "    try:\n",
    "        comment_count = int(soup.select_one('span.cmt_count').text.strip().replace('댓글', '').strip())\n",
    "    except:\n",
    "        comment_count = 0\n",
    "\n",
    "    try:\n",
    "        time_tag = soup.select_one('span.gall_date').text.strip()\n",
    "    except:\n",
    "        time_tag = \"\"\n",
    "\n",
    "    post_id = post_url.split('no=')[-1]\n",
    "    \n",
    "    comments = get_comments(post_id)\n",
    "    \n",
    "    return {\n",
    "        'post_id': post_id,\n",
    "        'title': title,\n",
    "        'body': body,\n",
    "        'like_count': like_count,\n",
    "        'comment_count': comment_count,\n",
    "        'time': time_tag,\n",
    "        'link': post_url,\n",
    "        'comments': comments\n",
    "    }\n",
    "\n",
    "# Step 3: 댓글 가져오기\n",
    "def get_comments(post_id):\n",
    "    comments = []\n",
    "    page = 1\n",
    "\n",
    "    while len(comments) < MAX_COMMENTS_PER_POST:\n",
    "        comment_url = f\"https://gall.dcinside.com/board/comment/lists/?id=all&no={post_id}&comment_page={page}\"\n",
    "        res = requests.get(comment_url)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        comment_list = soup.select('ul.comment_box > li')\n",
    "        \n",
    "        if not comment_list:\n",
    "            break  # 더 이상 댓글 없음\n",
    "        \n",
    "        for comment in comment_list:\n",
    "            content_tag = comment.select_one('p.usertxt')\n",
    "            if content_tag:\n",
    "                content = content_tag.text.strip()\n",
    "                comments.append(content)\n",
    "            \n",
    "            if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.1)  # 서버 부하 방지\n",
    "\n",
    "    return comments\n",
    "\n",
    "# 메인 크롤링\n",
    "def crawl_dc_main_popular():\n",
    "    post_links = get_main_popular_posts()\n",
    "    print(f\"Found {len(post_links)} popular posts.\")\n",
    "\n",
    "    for i, link in enumerate(post_links[:MAX_POSTS]):\n",
    "        print(f\"[{i+1}/{MAX_POSTS}] 크롤링 중: {link}\")\n",
    "        post = get_post_details(link)\n",
    "        posts.append(post)\n",
    "        time.sleep(0.5)  # 서버 부하 방지\n",
    "\n",
    "    # CSV 저장\n",
    "    df = pd.DataFrame(posts)\n",
    "    df.to_csv(f'dc_main_popular.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"CSV 저장 완료!\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_dc_main_popular()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12227226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 1 크롤링 중...\n",
      "[1] 실시간베스트 갤러리 이용 안내 크롤링 완료\n",
      "[2] 실시간베스트 갤러리 이용 안내 크롤링 완료\n",
      "[3] 전장연, 출근길 혜화역 지하철 시위… 또 강제 퇴거당해 크롤링 완료\n",
      "[4] 전장연, 출근길 혜화역 지하철 시위… 또 강제 퇴거당해 크롤링 완료\n",
      "[5] 공포의 인도 남성인권 운동 크롤링 완료\n",
      "[6] 공포의 인도 남성인권 운동 크롤링 완료\n",
      "[7] 전라도 군산시, 백종원 더본코리아에 70억 예산 투입 크롤링 완료\n",
      "[8] 전라도 군산시, 백종원 더본코리아에 70억 예산 투입 크롤링 완료\n",
      "[9] 마법과 마나의 세계에서 존재할리 없는 쿠노이치.1 크롤링 완료\n",
      "[10] 마법과 마나의 세계에서 존재할리 없는 쿠노이치.1 크롤링 완료\n",
      "[11] 김재연 \"여가부 장관 부총리로 격상, 비동간죄·차금법 통과, 탈원전\" 크롤링 완료\n",
      "[12] 김재연 \"여가부 장관 부총리로 격상, 비동간죄·차금법 통과, 탈원전\" 크롤링 완료\n",
      "[13] 씹덕 행사장에서 버튜버를 알아본 사람 크롤링 완료\n",
      "[14] 씹덕 행사장에서 버튜버를 알아본 사람 크롤링 완료\n",
      "[15] 싱글벙글 스타크래프트 IP경쟁 근황.jpg 크롤링 완료\n",
      "[16] 싱글벙글 스타크래프트 IP경쟁 근황.jpg 크롤링 완료\n",
      "[17] \"닦달하면 환불 없어\" '황당'문자..\"돈 떼이고 협박당하나\" 크롤링 완료\n",
      "[18] \"닦달하면 환불 없어\" '황당'문자..\"돈 떼이고 협박당하나\" 크롤링 완료\n",
      "[19] 한동훈 한화 이글스 뭐냐??????ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 크롤링 완료\n",
      "[20] 한동훈 한화 이글스 뭐냐??????ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 크롤링 완료\n",
      "[21] 싱글벙글 입으로 호흡 하면 안되는 이유 크롤링 완료\n",
      "[22] 싱글벙글 입으로 호흡 하면 안되는 이유 크롤링 완료\n",
      "[23] 어청도 2일차  탐조 크롤링 완료\n",
      "[24] 어청도 2일차  탐조 크롤링 완료\n",
      "[25] `이재명 측근` 정진상, `대장동 민간업자 재판`서 크롤링 완료\n",
      "[26] `이재명 측근` 정진상, `대장동 민간업자 재판`서 크롤링 완료\n",
      "[27] 여시)오늘자 한국여자와 결혼하면 안되는 이유 크롤링 완료\n",
      "[28] 여시)오늘자 한국여자와 결혼하면 안되는 이유 크롤링 완료\n",
      "[29] 김재환 PD 백종원 퇴로차단..jpg 크롤링 완료\n",
      "[30] 김재환 PD 백종원 퇴로차단..jpg 크롤링 완료\n",
      "[31] 오싹일본 지진 당시 쓰나미에 속수무책으로 당하는 자동차 안 상황.gif 크롤링 완료\n",
      "[32] 오싹일본 지진 당시 쓰나미에 속수무책으로 당하는 자동차 안 상황.gif 크롤링 완료\n",
      "[33] 대선 후보를 편의점 상품에 비유하는 김동연 크롤링 완료\n",
      "[34] 대선 후보를 편의점 상품에 비유하는 김동연 크롤링 완료\n",
      "[35] 초비상) 속보 SKT 복제폰으로 5000만원 털림 크롤링 완료\n",
      "[36] 초비상) 속보 SKT 복제폰으로 5000만원 털림 크롤링 완료\n",
      "[37] 싱글벙글 알고보면 중세 중국의 최고 사치품 크롤링 완료\n",
      "[38] 싱글벙글 알고보면 중세 중국의 최고 사치품 크롤링 완료\n",
      "[39] 미국에서 한국인간호사 채용거부 ㅈㄴ 웃기네 ㅋㅋㅋ 크롤링 완료\n",
      "[40] 미국에서 한국인간호사 채용거부 ㅈㄴ 웃기네 ㅋㅋㅋ 크롤링 완료\n",
      "[41] ⑤8박9일 도쿄-고치-나고야 여행기 크롤링 완료\n",
      "[42] ⑤8박9일 도쿄-고치-나고야 여행기 크롤링 완료\n",
      "[43] 싱글벙글 과거 1972년도 당시 서울과 도쿄의 모습 크롤링 완료\n",
      "[44] 싱글벙글 과거 1972년도 당시 서울과 도쿄의 모습 크롤링 완료\n",
      "[45] 서울 여의도 파크원 건물 화재로 대피 소동…인명 피해 없어 크롤링 완료\n",
      "[46] 서울 여의도 파크원 건물 화재로 대피 소동…인명 피해 없어 크롤링 완료\n",
      "[47] 소녀상 모욕한 유튜버 급기야... \"한국에 성병 퍼트리겠다\" 크롤링 완료\n",
      "[48] 소녀상 모욕한 유튜버 급기야... \"한국에 성병 퍼트리겠다\" 크롤링 완료\n",
      "[49] 4월 28일 시황 크롤링 완료\n",
      "[50] 4월 28일 시황 크롤링 완료\n",
      "[51] 유명 로판 작가 손 공개 대참사 ㄷㄷ 크롤링 완료\n",
      "[52] 유명 로판 작가 손 공개 대참사 ㄷㄷ 크롤링 완료\n",
      "[53] 싱글벙글 알힐랄 혼자 개털어버렸던 선수 크롤링 완료\n",
      "[54] 싱글벙글 알힐랄 혼자 개털어버렸던 선수 크롤링 완료\n",
      "[55] 이낙연, 대선 출마 뜻 밝혀…\"후보 등록 준비 시작\" 크롤링 완료\n",
      "[56] 이낙연, 대선 출마 뜻 밝혀…\"후보 등록 준비 시작\" 크롤링 완료\n",
      "[57] 강릉 주택가 군 포탄 폭발사고...3명 중경상 크롤링 완료\n",
      "[58] 강릉 주택가 군 포탄 폭발사고...3명 중경상 크롤링 완료\n",
      "[59] 냉혹한 재평가 받는 리더 캐릭터, 유바바의 세계.jpg 크롤링 완료\n",
      "[60] 냉혹한 재평가 받는 리더 캐릭터, 유바바의 세계.jpg 크롤링 완료\n",
      "[61] [솔카씹]수도권 인기 캠장 다녀옴 (축령산, 난지, 호명산) 크롤링 완료\n",
      "[62] [솔카씹]수도권 인기 캠장 다녀옴 (축령산, 난지, 호명산) 크롤링 완료\n",
      "[63] 트럼프 \"푸틴은 전쟁 멈춰야, 젤렌스키는 크림반도 영유권 포기 용의\" 크롤링 완료\n",
      "[64] 트럼프 \"푸틴은 전쟁 멈춰야, 젤렌스키는 크림반도 영유권 포기 용의\" 크롤링 완료\n",
      "[65] 또간집 안양편 논란 총정리 (+근황) 크롤링 완료\n",
      "[66] 또간집 안양편 논란 총정리 (+근황) 크롤링 완료\n",
      "[67] 북한, 러 파병 공식 확인…“김정은이 북러조약 근거해 결정” 크롤링 완료\n",
      "[68] 북한, 러 파병 공식 확인…“김정은이 북러조약 근거해 결정” 크롤링 완료\n",
      "[69] 현재 오버투어리즘으로 심각한 교토 근황 크롤링 완료\n",
      "[70] 현재 오버투어리즘으로 심각한 교토 근황 크롤링 완료\n",
      "[71] \"30~34세 미혼율 3배 늘었다\"디시글을 본 40대 더쿠 노괴 아줌마들 크롤링 완료\n",
      "[72] \"30~34세 미혼율 3배 늘었다\"디시글을 본 40대 더쿠 노괴 아줌마들 크롤링 완료\n",
      "[73] 이재명 오늘자 진짜 무섭네 ㄷㄷ.jpg 크롤링 완료\n",
      "[74] 이재명 오늘자 진짜 무섭네 ㄷㄷ.jpg 크롤링 완료\n",
      "[75] 보험도 '해킹' 뚫렸다…개인정보 유출 '비상' 크롤링 완료\n",
      "[76] 보험도 '해킹' 뚫렸다…개인정보 유출 '비상' 크롤링 완료\n",
      "[77] 싱글벙글 마리텔 열파참 성우 서유리 실시간 근황.jpg 크롤링 완료\n",
      "[78] 싱글벙글 마리텔 열파참 성우 서유리 실시간 근황.jpg 크롤링 완료\n",
      "[79] 검찰, 홈플러스·MBK파트너스 압수수색 크롤링 완료\n",
      "[80] 검찰, 홈플러스·MBK파트너스 압수수색 크롤링 완료\n",
      "[81] 싱글벙글 진격거를 본 어머니 반응.JPG 크롤링 완료\n",
      "[82] 싱글벙글 진격거를 본 어머니 반응.JPG 크롤링 완료\n",
      "[83] 아내가 불륜인 거 알면서도 흐린눈 하고 넘어가겠다는 퐁퐁이형 ㄷㄷ 크롤링 완료\n",
      "[84] 아내가 불륜인 거 알면서도 흐린눈 하고 넘어가겠다는 퐁퐁이형 ㄷㄷ 크롤링 완료\n",
      "[85] 싱글벙글 일일 서프라이즈!..manhwa 크롤링 완료\n",
      "[86] 싱글벙글 일일 서프라이즈!..manhwa 크롤링 완료\n",
      "[87] 청주 카페 사진 찍어왔어 보구가 크롤링 완료\n",
      "[88] 청주 카페 사진 찍어왔어 보구가 크롤링 완료\n",
      "[89] 싱글벙글 어제자 일본에서 알바테러 논란 크롤링 완료\n",
      "[90] 싱글벙글 어제자 일본에서 알바테러 논란 크롤링 완료\n",
      "[91] 조국, 옥중 편지 공개....jpg 크롤링 완료\n",
      "[92] 조국, 옥중 편지 공개....jpg 크롤링 완료\n",
      "[93] [금주의 신상] 4월 4주차 신제품 먹거리 모음.jpg 크롤링 완료\n",
      "[94] [금주의 신상] 4월 4주차 신제품 먹거리 모음.jpg 크롤링 완료\n",
      "[95] 리버풀 2025년 프리미어리그 우승! 크롤링 완료\n",
      "[96] 리버풀 2025년 프리미어리그 우승! 크롤링 완료\n",
      "[97] SKT 가입자 2300만명인데 유심 재고 100만개 크롤링 완료\n",
      "[98] SKT 가입자 2300만명인데 유심 재고 100만개 크롤링 완료\n",
      "[99] 밴쿠버 축제장에 차량 돌진으로 다수 사망 크롤링 완료\n",
      "[100] 밴쿠버 축제장에 차량 돌진으로 다수 사망 크롤링 완료\n",
      "\n",
      "✅ CSV 파일 저장 완료: dcinside_silbe_100posts.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 설정\n",
    "BASE_URL = \"https://gall.dcinside.com\"\n",
    "SILBE_URL = f\"{BASE_URL}/board/lists?id=dcbest\"\n",
    "MAX_POSTS = 100\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "POSTS_PER_PAGE = 20\n",
    "\n",
    "# requests 세션 생성\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Referer\": \"https://gall.dcinside.com/\"\n",
    "})\n",
    "\n",
    "results = []\n",
    "\n",
    "# 실베 페이지별 글 링크 가져오기\n",
    "def get_post_links(page_num):\n",
    "    url = f\"{SILBE_URL}&page={page_num}\"\n",
    "    res = session.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = []\n",
    "\n",
    "    # 'tr' 중에 class가 'ub-content us-post'인 것만 선택\n",
    "    posts = soup.select('tr.ub-content.us-post')\n",
    "\n",
    "    for post in posts:\n",
    "        link_tag = post.select_one('td.gall_tit a')\n",
    "        if link_tag:\n",
    "            href = link_tag.get(\"href\")\n",
    "            if href and '/board/view/' in href:\n",
    "                if href.startswith('http'):\n",
    "                    full_link = href\n",
    "                else:\n",
    "                    full_link = BASE_URL + href\n",
    "                links.append(full_link)\n",
    "    return links\n",
    "\n",
    "\n",
    "# 게시글 본문 + 댓글 가져오기\n",
    "def get_post_details(post_url):\n",
    "    res = session.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one(\"span.title_subject\").text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    try:\n",
    "        content = soup.select_one(\"div.write_div\").text.strip()\n",
    "    except:\n",
    "        content = \"본문 없음\"\n",
    "\n",
    "    comment_tags = soup.select(\"div.usertxt\")\n",
    "    comments = []\n",
    "    for comment_tag in comment_tags:\n",
    "        if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "            break\n",
    "        text = comment_tag.get_text(strip=True)\n",
    "        if text:\n",
    "            comments.append(text)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"link\": post_url,\n",
    "        \"content\": content,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "\n",
    "# 메인 크롤링 함수\n",
    "def crawl_silbe():\n",
    "    page = 1\n",
    "    crawled_posts = 0\n",
    "\n",
    "    while crawled_posts < MAX_POSTS:\n",
    "        print(f\"페이지 {page} 크롤링 중...\")\n",
    "        post_links = get_post_links(page)\n",
    "\n",
    "        for post_link in post_links:\n",
    "            if crawled_posts >= MAX_POSTS:\n",
    "                break\n",
    "            try:\n",
    "                post = get_post_details(post_link)\n",
    "                results.append(post)\n",
    "                crawled_posts += 1\n",
    "                print(f\"[{crawled_posts}] {post['title']} 크롤링 완료\")\n",
    "                time.sleep(0.5)  # 서버 부하 방지\n",
    "            except Exception as e:\n",
    "                print(f\"에러 발생 (패스): {e}\")\n",
    "                continue\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    # DataFrame으로 저장\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('dcinside_silbe_100posts.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\n✅ CSV 파일 저장 완료: dcinside_silbe_100posts.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_silbe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad47fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
