{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfe628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 설정\n",
    "BASE_URL = 'https://m.dcinside.com'\n",
    "GALLERY_ID = 'programming'  # 예시: 프로그래밍 갤러리 (네가 원하는 걸로 바꿔)\n",
    "MAX_POSTS = 100\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "\n",
    "# 결과 저장\n",
    "posts = []\n",
    "\n",
    "# Step 1: 인기글 리스트 가져오기\n",
    "def get_popular_posts(gallery_id):\n",
    "    url = f'{BASE_URL}/board/{gallery_id}?recommend=1'  # 개념글 목록\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    links = soup.select('tr.ub-content > td.gall_tit > a')\n",
    "    post_links = [BASE_URL + link['href'] for link in links]\n",
    "    return post_links\n",
    "\n",
    "# Step 2: 게시글 본문 + 댓글 가져오기\n",
    "def get_post_details(post_url):\n",
    "    res = requests.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one('div.ub-content > div.title_subject').text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    try:\n",
    "        body = soup.select_one('div.write_div').text.strip()\n",
    "    except:\n",
    "        body = \"\"\n",
    "\n",
    "    try:\n",
    "        like_count = int(soup.select_one('span.up_num').text.strip())\n",
    "    except:\n",
    "        like_count = 0\n",
    "\n",
    "    try:\n",
    "        comment_count = int(soup.select_one('span.cmt_count').text.strip().replace('댓글', '').strip())\n",
    "    except:\n",
    "        comment_count = 0\n",
    "\n",
    "    try:\n",
    "        time_tag = soup.select_one('div.gall_date')['title']\n",
    "    except:\n",
    "        time_tag = \"\"\n",
    "\n",
    "    post_id = post_url.split('/')[-1]\n",
    "    \n",
    "    comments = get_comments(post_id)\n",
    "    \n",
    "    return {\n",
    "        'post_id': post_id,\n",
    "        'title': title,\n",
    "        'body': body,\n",
    "        'like_count': like_count,\n",
    "        'comment_count': comment_count,\n",
    "        'time': time_tag,\n",
    "        'link': post_url,\n",
    "        'comments': comments\n",
    "    }\n",
    "\n",
    "# Step 3: 댓글 가져오기\n",
    "def get_comments(post_id):\n",
    "    comments = []\n",
    "    page = 1\n",
    "\n",
    "    while len(comments) < MAX_COMMENTS_PER_POST:\n",
    "        comment_url = f\"https://m.dcinside.com/board/comment/{post_id}?page={page}\"\n",
    "        res = requests.get(comment_url)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        comment_list = soup.select('ul.comment_box > li')\n",
    "        \n",
    "        if not comment_list:\n",
    "            break  # 더 이상 댓글 없음\n",
    "        \n",
    "        for comment in comment_list:\n",
    "            content_tag = comment.select_one('p.usertxt')\n",
    "            if content_tag:\n",
    "                content = content_tag.text.strip()\n",
    "                comments.append(content)\n",
    "            \n",
    "            if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(0.1)  # 서버 부하 줄이기\n",
    "\n",
    "    return comments\n",
    "\n",
    "# 메인 크롤링\n",
    "def crawl_dc_popular(gallery_id):\n",
    "    post_links = get_popular_posts(gallery_id)\n",
    "    print(f\"Found {len(post_links)} popular posts.\")\n",
    "    \n",
    "    for i, link in enumerate(post_links[:MAX_POSTS]):\n",
    "        print(f\"[{i+1}/{MAX_POSTS}] 크롤링 중: {link}\")\n",
    "        post = get_post_details(link)\n",
    "        posts.append(post)\n",
    "        time.sleep(0.5)  # 서버 부하 줄이기\n",
    "\n",
    "    # CSV 저장\n",
    "    df = pd.DataFrame(posts)\n",
    "    df.to_csv(f'dc_{gallery_id}_popular.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"CSV 저장 완료\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_dc_popular(GALLERY_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12227226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 1 크롤링 중...\n",
      "게시글 326081에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[1] 오늘 아침에 일어났던 청주시 고등학교 사건에 대한 기사 크롤링 완료\n",
      "게시글 326078에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[2] 준비란 물량까지 동났다...어르신들 공략한 매력적 제안 크롤링 완료\n",
      "게시글 326077에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[3] 첫해외여행&혼여 오사카-교토 5박6일 후기 크롤링 완료\n",
      "게시글 326074에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[4] 대구 북구 노곡동 함지산서 산불 발생…인근 주민·등산객 주의 크롤링 완료\n",
      "게시글 326072에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[5] 전장연, 출근길 혜화역 지하철 시위… 또 강제 퇴거당해 크롤링 완료\n",
      "게시글 326071에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[6] 공포의 인도 남성인권 운동 크롤링 완료\n",
      "게시글 326069에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[7] 전라도 군산시, 백종원 더본코리아에 70억 예산 투입 크롤링 완료\n",
      "게시글 326068에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[8] 마법과 마나의 세계에서 존재할리 없는 쿠노이치.1 크롤링 완료\n",
      "게시글 326066에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[9] 김재연 \"여가부 장관 부총리로 격상, 비동간죄·차금법 통과, 탈원전\" 크롤링 완료\n",
      "게시글 326065에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[10] 씹덕 행사장에서 버튜버를 알아본 사람 크롤링 완료\n",
      "게시글 326063에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[11] 싱글벙글 스타크래프트 IP경쟁 근황.jpg 크롤링 완료\n",
      "게시글 326062에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[12] \"닦달하면 환불 없어\" '황당'문자..\"돈 떼이고 협박당하나\" 크롤링 완료\n",
      "게시글 326060에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[13] 한동훈 한화 이글스 뭐냐??????ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 크롤링 완료\n",
      "게시글 326059에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[14] 싱글벙글 입으로 호흡 하면 안되는 이유 크롤링 완료\n",
      "게시글 326056에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[15] 어청도 2일차  탐조 크롤링 완료\n",
      "게시글 326054에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[16] `이재명 측근` 정진상, `대장동 민간업자 재판`서 크롤링 완료\n",
      "게시글 326053에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[17] 여시)오늘자 한국여자와 결혼하면 안되는 이유 크롤링 완료\n",
      "게시글 326051에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[18] 김재환 PD 백종원 퇴로차단..jpg 크롤링 완료\n",
      "게시글 326050에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[19] 오싹일본 지진 당시 쓰나미에 속수무책으로 당하는 자동차 안 상황.gif 크롤링 완료\n",
      "게시글 326048에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[20] 대선 후보를 편의점 상품에 비유하는 김동연 크롤링 완료\n",
      "게시글 326047에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[21] 초비상) 속보 SKT 복제폰으로 5000만원 털림 크롤링 완료\n",
      "게시글 326045에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[22] 싱글벙글 알고보면 중세 중국의 최고 사치품 크롤링 완료\n",
      "게시글 326042에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[23] 미국에서 한국인간호사 채용거부 ㅈㄴ 웃기네 ㅋㅋㅋ 크롤링 완료\n",
      "게시글 326041에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[24] ⑤8박9일 도쿄-고치-나고야 여행기 크롤링 완료\n",
      "게시글 326039에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[25] 싱글벙글 과거 1972년도 당시 서울과 도쿄의 모습 크롤링 완료\n",
      "게시글 326038에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[26] 서울 여의도 파크원 건물 화재로 대피 소동…인명 피해 없어 크롤링 완료\n",
      "게시글 326036에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[27] 소녀상 모욕한 유튜버 급기야... \"한국에 성병 퍼트리겠다\" 크롤링 완료\n",
      "게시글 326035에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[28] 4월 28일 시황 크롤링 완료\n",
      "게시글 326033에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[29] 유명 로판 작가 손 공개 대참사 ㄷㄷ 크롤링 완료\n",
      "게시글 326032에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[30] 싱글벙글 알힐랄 혼자 개털어버렸던 선수 크롤링 완료\n",
      "게시글 326030에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[31] 이낙연, 대선 출마 뜻 밝혀…\"후보 등록 준비 시작\" 크롤링 완료\n",
      "게시글 326029에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[32] 강릉 주택가 군 포탄 폭발사고...3명 중경상 크롤링 완료\n",
      "게시글 326026에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[33] 냉혹한 재평가 받는 리더 캐릭터, 유바바의 세계.jpg 크롤링 완료\n",
      "게시글 326024에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[34] [솔카씹]수도권 인기 캠장 다녀옴 (축령산, 난지, 호명산) 크롤링 완료\n",
      "게시글 326023에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[35] 트럼프 \"푸틴은 전쟁 멈춰야, 젤렌스키는 크림반도 영유권 포기 용의\" 크롤링 완료\n",
      "게시글 326021에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[36] 또간집 안양편 논란 총정리 (+근황) 크롤링 완료\n",
      "게시글 326020에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[37] 북한, 러 파병 공식 확인…“김정은이 북러조약 근거해 결정” 크롤링 완료\n",
      "게시글 326018에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[38] 현재 오버투어리즘으로 심각한 교토 근황 크롤링 완료\n",
      "게시글 326017에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[39] \"30~34세 미혼율 3배 늘었다\"디시글을 본 40대 더쿠 노괴 아줌마들 크롤링 완료\n",
      "게시글 326015에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[40] 이재명 오늘자 진짜 무섭네 ㄷㄷ.jpg 크롤링 완료\n",
      "게시글 326014에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[41] 보험도 '해킹' 뚫렸다…개인정보 유출 '비상' 크롤링 완료\n",
      "게시글 326012에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[42] 싱글벙글 마리텔 열파참 성우 서유리 실시간 근황.jpg 크롤링 완료\n",
      "게시글 326009에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[43] 검찰, 홈플러스·MBK파트너스 압수수색 크롤링 완료\n",
      "게시글 326008에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[44] 싱글벙글 진격거를 본 어머니 반응.JPG 크롤링 완료\n",
      "게시글 326006에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[45] 아내가 불륜인 거 알면서도 흐린눈 하고 넘어가겠다는 퐁퐁이형 ㄷㄷ 크롤링 완료\n",
      "게시글 326005에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[46] 싱글벙글 일일 서프라이즈!..manhwa 크롤링 완료\n",
      "게시글 326003에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[47] 청주 카페 사진 찍어왔어 보구가 크롤링 완료\n",
      "게시글 326002에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[48] 싱글벙글 어제자 일본에서 알바테러 논란 크롤링 완료\n",
      "게시글 326001에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[49] 조국, 옥중 편지 공개....jpg 크롤링 완료\n",
      "페이지 2 크롤링 중...\n",
      "게시글 325999에서 e_s_n_o를 찾을 수 없습니다.\n",
      "[50] [금주의 신상] 4월 4주차 신제품 먹거리 모음.jpg 크롤링 완료\n",
      "\n",
      "✅ CSV 파일 저장 완료: dcinside_silbe_100posts.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 설정\n",
    "BASE_URL = \"https://gall.dcinside.com\"\n",
    "SILBE_URL = f\"{BASE_URL}/board/lists?id=dcbest\"\n",
    "MAX_POSTS = 50\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "POSTS_PER_PAGE = 20\n",
    "\n",
    "# 세션 생성\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Referer\": \"https://gall.dcinside.com/\"\n",
    "})\n",
    "\n",
    "results = []\n",
    "crawled_post_ids = set()\n",
    "\n",
    "# 글 링크 가져오기 (공지/고정글 제외)\n",
    "def get_post_links(page_num):\n",
    "    url = f\"{SILBE_URL}&page={page_num}\"\n",
    "    res = session.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = []\n",
    "    \n",
    "    posts = soup.select('tr.ub-content.us-post')  # 공지 제외하고 진짜 글만\n",
    "    for post in posts:\n",
    "        link_tag = post.select_one('td.gall_tit a')\n",
    "        if link_tag:\n",
    "            href = link_tag.get(\"href\")\n",
    "            if href and '/board/view/' in href:\n",
    "                if href.startswith('http'):\n",
    "                    full_link = href\n",
    "                else:\n",
    "                    full_link = BASE_URL + href\n",
    "                links.append(full_link)\n",
    "    return links\n",
    "\n",
    "# 댓글 가져오기 (댓글 전용 API 호출)\n",
    "def get_comments(post_id):\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        # 게시글 페이지에서 필요한 정보 추출\n",
    "        post_url = f\"{BASE_URL}/board/view/?id=dcbest&no={post_id}\"\n",
    "        res = session.get(post_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        \n",
    "        # e_s_n_o 값을 찾기 (페이지 소스에서 이 값을 추출해야 함)\n",
    "        script_tags = soup.find_all('script')\n",
    "        e_s_n_o = None\n",
    "        for script in script_tags:\n",
    "            if script.string and 'e_s_n_o' in str(script.string):\n",
    "                match = re.search(r'e_s_n_o\\s*=\\s*\"([^\"]+)\"', str(script.string))\n",
    "                if match:\n",
    "                    e_s_n_o = match.group(1)\n",
    "                    break\n",
    "        \n",
    "        if not e_s_n_o:\n",
    "            print(f\"게시글 {post_id}에서 e_s_n_o를 찾을 수 없습니다.\")\n",
    "            return comments\n",
    "            \n",
    "        while len(comments) < MAX_COMMENTS_PER_POST:\n",
    "            # XHR 요청과 동일한 헤더 설정\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "                \"Referer\": post_url,\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                \"Accept\": \"application/json, text/javascript, */*; q=0.01\"\n",
    "            }\n",
    "            \n",
    "            # 댓글 API 요청 (새로운 형식)\n",
    "            comment_url = f\"{BASE_URL}/board/comment/\"\n",
    "            params = {\n",
    "                \"id\": \"dcbest\",\n",
    "                \"no\": post_id,\n",
    "                \"cmt_id\": \"dcbest\",\n",
    "                \"cmt_no\": post_id,\n",
    "                \"e_s_n_o\": e_s_n_o,\n",
    "                \"comment_page\": page\n",
    "            }\n",
    "            \n",
    "            res = session.get(comment_url, params=params, headers=headers)\n",
    "            \n",
    "            # JSON 응답 처리 시도\n",
    "            try:\n",
    "                json_data = res.json()\n",
    "                if 'comments' in json_data:\n",
    "                    for comment in json_data['comments']:\n",
    "                        if 'comment' in comment:\n",
    "                            comments.append(comment['comment'])\n",
    "                        if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                            break\n",
    "                else:\n",
    "                    # JSON 형식이지만 comments 키가 없는 경우\n",
    "                    break\n",
    "            except:\n",
    "                # JSON 파싱 실패 - HTML 방식으로 시도\n",
    "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "                comment_tags = soup.select('.comment_box .comment_text')\n",
    "                \n",
    "                if not comment_tags:\n",
    "                    break\n",
    "                    \n",
    "                for comment_tag in comment_tags:\n",
    "                    text = comment_tag.get_text(strip=True)\n",
    "                    if text:\n",
    "                        comments.append(text)\n",
    "                    if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                        break\n",
    "            \n",
    "            # 다음 페이지가 없거나 충분한 댓글을 모았으면 종료\n",
    "            if len(comments) < (page * 20) or len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(0.2)  # 조금 더 긴 대기 시간\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"댓글 수집 오류 (게시글 {post_id}): {e}\")\n",
    "        \n",
    "    return comments\n",
    "\n",
    "\n",
    "# 글 본문 + 댓글 가져오기\n",
    "def get_post_details(post_url):\n",
    "    res = session.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one(\"span.title_subject\").text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    try:\n",
    "        content = soup.select_one(\"div.write_div\").text.strip()\n",
    "    except:\n",
    "        content = \"본문 없음\"\n",
    "\n",
    "    try:\n",
    "        post_id = post_url.split('no=')[1].split('&')[0]\n",
    "    except:\n",
    "        post_id = \"\"\n",
    "\n",
    "    comments = get_comments(post_id)\n",
    "\n",
    "    return {\n",
    "        \"post_id\": post_id,\n",
    "        \"title\": title,\n",
    "        \"link\": post_url,\n",
    "        \"content\": content,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "\n",
    "# 메인 크롤링 함수\n",
    "def crawl_silbe():\n",
    "    page = 1\n",
    "    crawled_posts = 0\n",
    "\n",
    "    while crawled_posts < MAX_POSTS:\n",
    "        print(f\"페이지 {page} 크롤링 중...\")\n",
    "        post_links = get_post_links(page)\n",
    "\n",
    "        for post_link in post_links:\n",
    "            if crawled_posts >= MAX_POSTS:\n",
    "                break\n",
    "\n",
    "            post_id = post_link.split('no=')[1].split('&')[0]\n",
    "            if post_id in crawled_post_ids:\n",
    "                continue  # 이미 크롤링한 글이면 스킵\n",
    "\n",
    "            try:\n",
    "                post = get_post_details(post_link)\n",
    "                results.append(post)\n",
    "                crawled_post_ids.add(post_id)\n",
    "                crawled_posts += 1\n",
    "                print(f\"[{crawled_posts}] {post['title']} 크롤링 완료\")\n",
    "                time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"에러 발생 (패스): {e}\")\n",
    "                continue\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    # 결과 저장\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('dcinside_silbe_100posts.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\n✅ CSV 파일 저장 완료: dcinside_silbe_100posts.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_silbe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baad030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 1 크롤링 중...\n",
      "게시글 326084 크롤링 중...\n",
      "[1/50] SKT 쓰는 임원, 빨리 바꿔라\" 대기업도 '비상'…커지는 불안 크롤링 완료 (댓글 14개)\n",
      "게시글 326083 크롤링 중...\n",
      "[2/50] 롯데 우승까지 유튜브한다는 아로치카 근황..jpg 크롤링 완료 (댓글 17개)\n",
      "게시글 326081 크롤링 중...\n",
      "[3/50] 오늘 아침에 일어났던 청주시 고등학교 사건에 대한 기사 크롤링 완료 (댓글 50개)\n",
      "게시글 326078 크롤링 중...\n",
      "[4/50] 준비란 물량까지 동났다...어르신들 공략한 매력적 제안 크롤링 완료 (댓글 50개)\n",
      "게시글 326077 크롤링 중...\n",
      "[5/50] 첫해외여행&혼여 오사카-교토 5박6일 후기 크롤링 완료 (댓글 4개)\n",
      "게시글 326074 크롤링 중...\n",
      "[6/50] 대구 북구 노곡동 함지산서 산불 발생…인근 주민·등산객 주의 크롤링 완료 (댓글 50개)\n",
      "게시글 326072 크롤링 중...\n",
      "[7/50] 전장연, 출근길 혜화역 지하철 시위… 또 강제 퇴거당해 크롤링 완료 (댓글 50개)\n",
      "게시글 326071 크롤링 중...\n",
      "[8/50] 공포의 인도 남성인권 운동 크롤링 완료 (댓글 50개)\n",
      "게시글 326069 크롤링 중...\n",
      "[9/50] 전라도 군산시, 백종원 더본코리아에 70억 예산 투입 크롤링 완료 (댓글 50개)\n",
      "게시글 326068 크롤링 중...\n",
      "[10/50] 마법과 마나의 세계에서 존재할리 없는 쿠노이치.1 크롤링 완료 (댓글 35개)\n",
      "게시글 326066 크롤링 중...\n",
      "[11/50] 김재연 \"여가부 장관 부총리로 격상, 비동간죄·차금법 통과, 탈원전\" 크롤링 완료 (댓글 50개)\n",
      "게시글 326065 크롤링 중...\n",
      "[12/50] 씹덕 행사장에서 버튜버를 알아본 사람 크롤링 완료 (댓글 50개)\n",
      "게시글 326063 크롤링 중...\n",
      "[13/50] 싱글벙글 스타크래프트 IP경쟁 근황.jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326062 크롤링 중...\n",
      "[14/50] \"닦달하면 환불 없어\" '황당'문자..\"돈 떼이고 협박당하나\" 크롤링 완료 (댓글 50개)\n",
      "게시글 326060 크롤링 중...\n",
      "[15/50] 한동훈 한화 이글스 뭐냐??????ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 크롤링 완료 (댓글 50개)\n",
      "게시글 326059 크롤링 중...\n",
      "[16/50] 싱글벙글 입으로 호흡 하면 안되는 이유 크롤링 완료 (댓글 50개)\n",
      "게시글 326056 크롤링 중...\n",
      "[17/50] 어청도 2일차  탐조 크롤링 완료 (댓글 16개)\n",
      "게시글 326054 크롤링 중...\n",
      "[18/50] `이재명 측근` 정진상, `대장동 민간업자 재판`서 크롤링 완료 (댓글 50개)\n",
      "게시글 326053 크롤링 중...\n",
      "[19/50] 여시)오늘자 한국여자와 결혼하면 안되는 이유 크롤링 완료 (댓글 50개)\n",
      "게시글 326051 크롤링 중...\n",
      "[20/50] 김재환 PD 백종원 퇴로차단..jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326050 크롤링 중...\n",
      "[21/50] 오싹일본 지진 당시 쓰나미에 속수무책으로 당하는 자동차 안 상황.gif 크롤링 완료 (댓글 50개)\n",
      "게시글 326048 크롤링 중...\n",
      "[22/50] 대선 후보를 편의점 상품에 비유하는 김동연 크롤링 완료 (댓글 50개)\n",
      "게시글 326047 크롤링 중...\n",
      "[23/50] 초비상) 속보 SKT 복제폰으로 5000만원 털림 크롤링 완료 (댓글 50개)\n",
      "게시글 326045 크롤링 중...\n",
      "[24/50] 싱글벙글 알고보면 중세 중국의 최고 사치품 크롤링 완료 (댓글 50개)\n",
      "게시글 326042 크롤링 중...\n",
      "[25/50] 미국에서 한국인간호사 채용거부 ㅈㄴ 웃기네 ㅋㅋㅋ 크롤링 완료 (댓글 50개)\n",
      "게시글 326041 크롤링 중...\n",
      "[26/50] ⑤8박9일 도쿄-고치-나고야 여행기 크롤링 완료 (댓글 9개)\n",
      "게시글 326039 크롤링 중...\n",
      "[27/50] 싱글벙글 과거 1972년도 당시 서울과 도쿄의 모습 크롤링 완료 (댓글 50개)\n",
      "게시글 326038 크롤링 중...\n",
      "[28/50] 서울 여의도 파크원 건물 화재로 대피 소동…인명 피해 없어 크롤링 완료 (댓글 39개)\n",
      "게시글 326036 크롤링 중...\n",
      "[29/50] 소녀상 모욕한 유튜버 급기야... \"한국에 성병 퍼트리겠다\" 크롤링 완료 (댓글 50개)\n",
      "게시글 326035 크롤링 중...\n",
      "[30/50] 4월 28일 시황 크롤링 완료 (댓글 6개)\n",
      "게시글 326033 크롤링 중...\n",
      "[31/50] 유명 로판 작가 손 공개 대참사 ㄷㄷ 크롤링 완료 (댓글 50개)\n",
      "게시글 326032 크롤링 중...\n",
      "[32/50] 싱글벙글 알힐랄 혼자 개털어버렸던 선수 크롤링 완료 (댓글 31개)\n",
      "게시글 326030 크롤링 중...\n",
      "[33/50] 이낙연, 대선 출마 뜻 밝혀…\"후보 등록 준비 시작\" 크롤링 완료 (댓글 50개)\n",
      "게시글 326029 크롤링 중...\n",
      "[34/50] 강릉 주택가 군 포탄 폭발사고...3명 중경상 크롤링 완료 (댓글 50개)\n",
      "게시글 326026 크롤링 중...\n",
      "[35/50] 냉혹한 재평가 받는 리더 캐릭터, 유바바의 세계.jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326024 크롤링 중...\n",
      "[36/50] [솔카씹]수도권 인기 캠장 다녀옴 (축령산, 난지, 호명산) 크롤링 완료 (댓글 32개)\n",
      "게시글 326023 크롤링 중...\n",
      "[37/50] 트럼프 \"푸틴은 전쟁 멈춰야, 젤렌스키는 크림반도 영유권 포기 용의\" 크롤링 완료 (댓글 50개)\n",
      "게시글 326021 크롤링 중...\n",
      "[38/50] 또간집 안양편 논란 총정리 (+근황) 크롤링 완료 (댓글 50개)\n",
      "게시글 326020 크롤링 중...\n",
      "[39/50] 북한, 러 파병 공식 확인…“김정은이 북러조약 근거해 결정” 크롤링 완료 (댓글 50개)\n",
      "게시글 326018 크롤링 중...\n",
      "[40/50] 현재 오버투어리즘으로 심각한 교토 근황 크롤링 완료 (댓글 50개)\n",
      "게시글 326017 크롤링 중...\n",
      "[41/50] \"30~34세 미혼율 3배 늘었다\"디시글을 본 40대 더쿠 노괴 아줌마들 크롤링 완료 (댓글 50개)\n",
      "게시글 326015 크롤링 중...\n",
      "[42/50] 이재명 오늘자 진짜 무섭네 ㄷㄷ.jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326014 크롤링 중...\n",
      "[43/50] 보험도 '해킹' 뚫렸다…개인정보 유출 '비상' 크롤링 완료 (댓글 50개)\n",
      "게시글 326012 크롤링 중...\n",
      "[44/50] 싱글벙글 마리텔 열파참 성우 서유리 실시간 근황.jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326009 크롤링 중...\n",
      "[45/50] 검찰, 홈플러스·MBK파트너스 압수수색 크롤링 완료 (댓글 40개)\n",
      "게시글 326008 크롤링 중...\n",
      "[46/50] 싱글벙글 진격거를 본 어머니 반응.JPG 크롤링 완료 (댓글 50개)\n",
      "게시글 326006 크롤링 중...\n",
      "[47/50] 아내가 불륜인 거 알면서도 흐린눈 하고 넘어가겠다는 퐁퐁이형 ㄷㄷ 크롤링 완료 (댓글 50개)\n",
      "게시글 326005 크롤링 중...\n",
      "[48/50] 싱글벙글 일일 서프라이즈!..manhwa 크롤링 완료 (댓글 23개)\n",
      "게시글 326003 크롤링 중...\n",
      "[49/50] 청주 카페 사진 찍어왔어 보구가 크롤링 완료 (댓글 36개)\n",
      "페이지 2 크롤링 중...\n",
      "게시글 326002 크롤링 중...\n",
      "[50/50] 싱글벙글 어제자 일본에서 알바테러 논란 크롤링 완료 (댓글 50개)\n",
      "\n",
      "✅ CSV 파일 저장 완료: dcinside_silbe_posts.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# 설정\n",
    "BASE_URL = \"https://gall.dcinside.com\"\n",
    "SILBE_URL = f\"{BASE_URL}/board/lists?id=dcbest\"\n",
    "MAX_POSTS = 50\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "POSTS_PER_PAGE = 20\n",
    "\n",
    "# User-Agent 목록\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\"\n",
    "]\n",
    "\n",
    "# 세션 생성\n",
    "session = requests.Session()\n",
    "\n",
    "# 세션 헤더 랜덤 설정 함수\n",
    "def update_session_headers():\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": random.choice(USER_AGENTS),\n",
    "        \"Referer\": \"https://gall.dcinside.com/\"\n",
    "    })\n",
    "\n",
    "update_session_headers()\n",
    "\n",
    "results = []\n",
    "crawled_post_ids = set()\n",
    "\n",
    "# 글 링크 가져오기 (공지/고정글 제외)\n",
    "def get_post_links(page_num):\n",
    "    update_session_headers()\n",
    "    url = f\"{SILBE_URL}&page={page_num}\"\n",
    "    try:\n",
    "        res = session.get(url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        posts = soup.select('tr.ub-content.us-post')\n",
    "        links = []\n",
    "        for post in posts:\n",
    "            link_tag = post.select_one('td.gall_tit a')\n",
    "            if link_tag:\n",
    "                href = link_tag.get(\"href\")\n",
    "                if href and '/board/view/' in href:\n",
    "                    full_link = href if href.startswith('http') else BASE_URL + href\n",
    "                    # 게시글 ID 추출\n",
    "                    try:\n",
    "                        post_id = href.split('no=')[1].split('&')[0]\n",
    "                    except:\n",
    "                        post_id = \"\"\n",
    "                    # 작성시간, 추천수 추출\n",
    "                    try:\n",
    "                        write_time = post.select_one('td.gall_date').text.strip()\n",
    "                    except:\n",
    "                        write_time = \"\"\n",
    "                    try:\n",
    "                        recommend = post.select_one('td.gall_recommend').text.strip()\n",
    "                    except:\n",
    "                        recommend = \"\"\n",
    "                    links.append({\n",
    "                        \"link\": full_link,\n",
    "                        \"post_id\": post_id,\n",
    "                        \"write_time\": write_time,\n",
    "                        \"recommend\": recommend\n",
    "                    })\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"글 목록 가져오기 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# 댓글 가져오기 (API 요청 방식)\n",
    "def get_comments_api(post_id):\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        # 게시글 페이지에서 필요한 정보 추출\n",
    "        update_session_headers()\n",
    "        post_url = f\"{BASE_URL}/board/view/?id=dcbest&no={post_id}\"\n",
    "        res = session.get(post_url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        \n",
    "        # e_s_n_o 값을 찾기 (페이지 소스에서 이 값을 추출해야 함)\n",
    "        script_tags = soup.find_all('script')\n",
    "        e_s_n_o = None\n",
    "        for script in script_tags:\n",
    "            if script.string and 'e_s_n_o' in str(script.string):\n",
    "                match = re.search(r'e_s_n_o\\s*=\\s*\"([^\"]+)\"', str(script.string))\n",
    "                if match:\n",
    "                    e_s_n_o = match.group(1)\n",
    "                    break\n",
    "        \n",
    "        if not e_s_n_o:\n",
    "            print(f\"게시글 {post_id}에서 e_s_n_o를 찾을 수 없습니다. 다른 방식으로 시도합니다.\")\n",
    "            return get_comments_html(post_id, soup)\n",
    "            \n",
    "        while len(comments) < MAX_COMMENTS_PER_POST:\n",
    "            update_session_headers()\n",
    "            \n",
    "            # 댓글 API 요청\n",
    "            comment_url = f\"{BASE_URL}/board/comment/\"\n",
    "            params = {\n",
    "                \"id\": \"dcbest\",\n",
    "                \"no\": post_id,\n",
    "                \"cmt_id\": \"dcbest\",\n",
    "                \"cmt_no\": post_id,\n",
    "                \"e_s_n_o\": e_s_n_o,\n",
    "                \"comment_page\": page\n",
    "            }\n",
    "            \n",
    "            headers = session.headers.copy()\n",
    "            headers.update({\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "                \"Referer\": post_url\n",
    "            })\n",
    "            \n",
    "            res = session.get(comment_url, params=params, headers=headers)\n",
    "            \n",
    "            # JSON 응답 처리 시도\n",
    "            try:\n",
    "                json_data = res.json()\n",
    "                if 'comments' in json_data:\n",
    "                    found_comments = False\n",
    "                    for comment in json_data['comments']:\n",
    "                        if 'comment' in comment:\n",
    "                            comments.append(comment['comment'])\n",
    "                            found_comments = True\n",
    "                        if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                            break\n",
    "                    if not found_comments:\n",
    "                        break\n",
    "                else:\n",
    "                    # JSON 형식이지만 comments 키가 없는 경우\n",
    "                    break\n",
    "            except:\n",
    "                # JSON 파싱 실패 시 HTML 방식으로 전환\n",
    "                print(f\"JSON 파싱 실패. HTML 방식으로 전환합니다.\")\n",
    "                html_comments = get_comments_html(post_id, None)\n",
    "                comments.extend(html_comments)\n",
    "                break\n",
    "            \n",
    "            # 다음 페이지가 없거나 충분한 댓글을 모았으면 종료\n",
    "            if len(comments) < (page * 20) or len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(random.uniform(0.5, 1.0))  # 랜덤 지연\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"API 댓글 수집 오류 (게시글 {post_id}): {e}\")\n",
    "        # 오류 발생 시 HTML 방식으로 시도\n",
    "        html_comments = get_comments_html(post_id, None)\n",
    "        comments.extend(html_comments)\n",
    "        \n",
    "    return comments\n",
    "\n",
    "# HTML 파싱 방식으로 댓글 가져오기 (대체 방식)\n",
    "def get_comments_html(post_id, existing_soup=None):\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        if existing_soup is None:\n",
    "            update_session_headers()\n",
    "            post_url = f\"{BASE_URL}/board/view/?id=dcbest&no={post_id}\"\n",
    "            res = session.get(post_url)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        else:\n",
    "            soup = existing_soup\n",
    "        \n",
    "        # 여러 가능한 CSS 선택자로 시도\n",
    "        selectors = [\n",
    "            'div.comment_box div.usertxt',\n",
    "            'div.comment_box span.comment_text',\n",
    "            'div.reply_box div.reply_text',\n",
    "            'p.usertxt',\n",
    "            'div.comment_view p.inner_info'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            comment_tags = soup.select(selector)\n",
    "            if comment_tags:\n",
    "                for comment_tag in comment_tags:\n",
    "                    text = comment_tag.get_text(strip=True)\n",
    "                    if text:\n",
    "                        comments.append(text)\n",
    "                    if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                        break\n",
    "                break  # 성공한 선택자가 있으면 루프 종료\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"HTML 댓글 수집 오류 (게시글 {post_id}): {e}\")\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Selenium을 이용한 댓글 크롤링 (최후의 수단)\n",
    "def get_comments_selenium(post_id):\n",
    "    comments = []\n",
    "    \n",
    "    # Chrome 옵션 설정\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # 헤드리스 모드\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')\n",
    "    \n",
    "    # 웹드라이버 초기화\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    try:\n",
    "        # 게시글 페이지 접속\n",
    "        post_url = f\"{BASE_URL}/board/view/?id=dcbest&no={post_id}\"\n",
    "        driver.get(post_url)\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "        \n",
    "        # 댓글 영역의 여러 가능한 선택자\n",
    "        selectors = [\n",
    "            \".comment_box .comment_text\",\n",
    "            \".comment_box .usertxt\",\n",
    "            \".reply_box .reply_text\"\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            comment_elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if comment_elements:\n",
    "                for element in comment_elements:\n",
    "                    text = element.text.strip()\n",
    "                    if text:\n",
    "                        comments.append(text)\n",
    "                    if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                        break\n",
    "                break  # 성공한 선택자가 있으면 루프 종료\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Selenium 댓글 수집 오류 (게시글 {post_id}): {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "    return comments\n",
    "\n",
    "def get_comments(post_id):\n",
    "    # 무조건 Selenium 사용\n",
    "    return get_comments_selenium(post_id)\n",
    "\n",
    "def get_comments_selenium(post_id):\n",
    "    comments = []\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    try:\n",
    "        post_url = f\"{BASE_URL}/board/view/?id=dcbest&no={post_id}\"\n",
    "        driver.get(post_url)\n",
    "        time.sleep(2)  # 충분한 로딩 대기\n",
    "\n",
    "        # 댓글이 여러 페이지에 걸쳐 있을 수 있으므로 반복\n",
    "        while True:\n",
    "            # 댓글 텍스트 추출\n",
    "            comment_elements = driver.find_elements(By.CLASS_NAME, \"usertxt\")\n",
    "            for elem in comment_elements:\n",
    "                text = elem.text.strip()\n",
    "                if text and text not in comments:\n",
    "                    comments.append(text)\n",
    "                if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                    break\n",
    "            if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "\n",
    "            # \"더보기\" 버튼 또는 다음 페이지 버튼 찾기 (없으면 종료)\n",
    "            try:\n",
    "                more_btn = driver.find_element(By.CSS_SELECTOR, \".comment_more_btn\")\n",
    "                if more_btn.is_displayed():\n",
    "                    more_btn.click()\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                break  # 더 이상 버튼이 없으면 종료\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Selenium 댓글 수집 오류 (게시글 {post_id}): {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return comments\n",
    "\n",
    "\n",
    "# 글 본문 + 댓글 가져오기\n",
    "def get_post_details(post_url):\n",
    "    update_session_headers()\n",
    "    res = session.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one(\"span.title_subject\").text.strip()\n",
    "    except:\n",
    "        title = \"제목 없음\"\n",
    "\n",
    "    try:\n",
    "        content = soup.select_one(\"div.write_div\").text.strip()\n",
    "    except:\n",
    "        content = \"본문 없음\"\n",
    "\n",
    "    try:\n",
    "        post_id = post_url.split('no=')[1].split('&')[0]\n",
    "    except:\n",
    "        post_id = \"\"\n",
    "\n",
    "    comments = get_comments(post_id)\n",
    "\n",
    "    return {\n",
    "        \"post_id\": post_id,\n",
    "        \"title\": title,\n",
    "        \"link\": post_url,\n",
    "        \"content\": content,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "\n",
    "# 메인 크롤링 함수\n",
    "def crawl_silbe():\n",
    "    USE_SELENIUM = False  # Selenium 사용 여부 설정\n",
    "    page = 1\n",
    "    crawled_posts = 0\n",
    "\n",
    "    while crawled_posts < MAX_POSTS:\n",
    "        print(f\"페이지 {page} 크롤링 중...\")\n",
    "        post_links = get_post_links(page)\n",
    "\n",
    "        if not post_links:\n",
    "            print(f\"페이지 {page}에서 글을 찾을 수 없습니다. 다음 페이지로 넘어갑니다.\")\n",
    "            page += 1\n",
    "            time.sleep(random.uniform(1.0, 2.0))\n",
    "            continue\n",
    "\n",
    "        for post_link in post_links:\n",
    "            if crawled_posts >= MAX_POSTS:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                post_id = post_link.split('no=')[1].split('&')[0]\n",
    "            except:\n",
    "                print(f\"링크에서 게시글 ID를 추출할 수 없습니다: {post_link}\")\n",
    "                continue\n",
    "                \n",
    "            if post_id in crawled_post_ids:\n",
    "                continue  # 이미 크롤링한 글이면 스킵\n",
    "\n",
    "            try:\n",
    "                print(f\"게시글 {post_id} 크롤링 중...\")\n",
    "                post = get_post_details(post_link)\n",
    "                results.append(post)\n",
    "                crawled_post_ids.add(post_id)\n",
    "                crawled_posts += 1\n",
    "                print(f\"[{crawled_posts}/{MAX_POSTS}] {post['title']} 크롤링 완료 (댓글 {len(post['comments'])}개)\")\n",
    "                time.sleep(random.uniform(0.8, 1.5))  # 랜덤 지연\n",
    "            except Exception as e:\n",
    "                print(f\"게시글 크롤링 오류 (패스): {e}\")\n",
    "                continue\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(random.uniform(1.5, 3.0))  # 페이지 간 랜덤 지연\n",
    "\n",
    "    # 결과 저장\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('dcinside_silbe_posts.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\n✅ CSV 파일 저장 완료: dcinside_silbe_posts.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_silbe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f793b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 1 크롤링 중...\n",
      "게시글 326253 크롤링 중...\n",
      "[1/50] 전세계에 난리난 AI 답변 크롤링 완료 (댓글 5개)\n",
      "게시글 326251 크롤링 중...\n",
      "[2/50] 왜노자 일본 스위치2 체험회 갔다온 후기 크롤링 완료 (댓글 44개)\n",
      "게시글 326249 크롤링 중...\n",
      "[3/50] 6시에 저녁식사하는 한국이 신기하다는 서양인들 크롤링 완료 (댓글 44개)\n",
      "게시글 326247 크롤링 중...\n",
      "[4/50] 도쿄 3박 4일로 여행아다 뗌. 3일차 (1) 크롤링 완료 (댓글 13개)\n",
      "게시글 326246 크롤링 중...\n",
      "[5/50] 신문고답변) 서울가락몰옥토버페스타 위법 인정 크롤링 완료 (댓글 24개)\n",
      "게시글 326241 크롤링 중...\n",
      "[6/50] 퍼거슨이 박지성에게 쓴 편지.....jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326239 크롤링 중...\n",
      "[7/50] ④ 9박 10일 일본 배낭여행기 4일차 -1,2 (모리오카,아키타,4월18일) 크롤링 완료 (댓글 7개)\n",
      "게시글 326237 크롤링 중...\n",
      "[8/50] 정동원 중2병 완치시켜준 장민호 크롤링 완료 (댓글 50개)\n",
      "게시글 326235 크롤링 중...\n",
      "[9/50] 백종원이 가위질 솔루션을 해준 이유 크롤링 완료 (댓글 50개)\n",
      "게시글 326234 크롤링 중...\n",
      "[10/50] 싱글벙글 스페이드 에이스 카드가 유독 특별한 이유.jpg 크롤링 완료 (댓글 46개)\n",
      "게시글 326232 크롤링 중...\n",
      "[11/50] 빨대로 빨아먹는 비빔면이 있다? 크롤링 완료 (댓글 50개)\n",
      "게시글 326230 크롤링 중...\n",
      "[12/50] 한복은 별로 입고 싶지 않다는 탈북녀 크롤링 완료 (댓글 50개)\n",
      "게시글 326228 크롤링 중...\n",
      "[13/50] 전광훈 후보님 공약 2탄 26~50 크롤링 완료 (댓글 50개)\n",
      "게시글 326226 크롤링 중...\n",
      "[14/50] 중국 춘추전국시대 보물 수준 크롤링 완료 (댓글 50개)\n",
      "게시글 326224 크롤링 중...\n",
      "[15/50] 응급실에 실려간 형 크롤링 완료 (댓글 50개)\n",
      "게시글 326220 크롤링 중...\n",
      "[16/50] 부산 재건 힘썼던 위트컴 장군, 부산대서 충혼비 논란 크롤링 완료 (댓글 50개)\n",
      "게시글 326218 크롤링 중...\n",
      "[17/50] 알쏭달쏭 전남대학교 에타촌 크롤링 완료 (댓글 50개)\n",
      "게시글 326216 크롤링 중...\n",
      "[18/50] 싸우면 질것같은 외국누나 크롤링 완료 (댓글 50개)\n",
      "게시글 326214 크롤링 중...\n",
      "[19/50] 여초주의) 안정환에게 플러팅하는 김태균 크롤링 완료 (댓글 50개)\n",
      "게시글 326212 크롤링 중...\n",
      "[20/50] 여교수와 연구실에 단 둘이 남아버린 manhwa 크롤링 완료 (댓글 50개)\n",
      "게시글 326210 크롤링 중...\n",
      "[21/50] 냉혹한 번데기 분석의 세계.jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326208 크롤링 중...\n",
      "[22/50] 여시)오늘자 곧 결혼시장 쏟아져나올 30대한녀님들~~~ 크롤링 완료 (댓글 50개)\n",
      "게시글 326206 크롤링 중...\n",
      "[23/50] [회의록] 통영시, 49억+100억 쏜다(외식산업개발원, 수산특화마을) 크롤링 완료 (댓글 50개)\n",
      "게시글 326204 크롤링 중...\n",
      "[24/50] 교사 수당,급여 인상 절실합니다.blind 크롤링 완료 (댓글 50개)\n",
      "게시글 326200 크롤링 중...\n",
      "[25/50] 펜탁스KM 후지400 평화로운 시즈오카 동네 스냅 크롤링 완료 (댓글 22개)\n",
      "게시글 326198 크롤링 중...\n",
      "[26/50] 블라) 37세 남성은 몇살까지 노려볼 수 잇을까요? 크롤링 완료 (댓글 50개)\n",
      "게시글 326196 크롤링 중...\n",
      "[27/50] 스압) Skt 사태 보니까 갑자기 떠오른거 크롤링 완료 (댓글 50개)\n",
      "게시글 326194 크롤링 중...\n",
      "[28/50] 3050 남성들 위험하다…밥벌이의 버거움, 자살 10년래 최대 크롤링 완료 (댓글 50개)\n",
      "게시글 326192 크롤링 중...\n",
      "[29/50] 절대 그러면 안되는걸 알지만 여친 폰을 몰래 보고야 말았어... 크롤링 완료 (댓글 50개)\n",
      "게시글 326190 크롤링 중...\n",
      "[30/50] 러시아군 소속으로 전사한 CIA 부국장 아들의 생전 사진들이 발굴됨 크롤링 완료 (댓글 50개)\n",
      "게시글 326188 크롤링 중...\n",
      "[31/50] 싱글벙글 한 쇼츠에 올라온 대한민국 '중산층' 특징 크롤링 완료 (댓글 50개)\n",
      "게시글 326186 크롤링 중...\n",
      "[32/50] ???:헌혈이 무슨 자랑스러운 일인줄 아냐? 크롤링 완료 (댓글 50개)\n",
      "게시글 326184 크롤링 중...\n",
      "[33/50] 프렌차이즈 시장을 덮쳐오는 소송 열풍… 흔들리는 기업들 크롤링 완료 (댓글 50개)\n",
      "게시글 326182 크롤링 중...\n",
      "[34/50] 전문대 에타로 보는 요즘 대학생들 크롤링 완료 (댓글 50개)\n",
      "게시글 326180 크롤링 중...\n",
      "[35/50] 백종원 위인전 만화속 발명한것과 성공신화 업적들...jpg 크롤링 완료 (댓글 50개)\n",
      "게시글 326176 크롤링 중...\n",
      "[36/50] 의대 못 간 아들 때문에 술 먹다 죽은 아버지 크롤링 완료 (댓글 50개)\n",
      "게시글 326174 크롤링 중...\n",
      "[37/50] AI의 발전으로 승정원일기 20~30년내 복구 가능하다는 안철수 크롤링 완료 (댓글 50개)\n",
      "게시글 326172 크롤링 중...\n",
      "[38/50] 나이먹고 젊은 30대 여자한테 들이대지 좀 말라는 블라녀 ㄷㄷ 크롤링 완료 (댓글 50개)\n",
      "게시글 326170 크롤링 중...\n",
      "[39/50] 미국, 동물실험 폐지 수순…한국 과학자, 오가노이드로 감각신경회로 구현 크롤링 완료 (댓글 50개)\n",
      "게시글 326168 크롤링 중...\n",
      "[40/50] 기분좋은 갤에 풀어보는 이정후 직관 후기 크롤링 완료 (댓글 26개)\n",
      "게시글 326166 크롤링 중...\n",
      "[41/50] 중동에서 '한국 책' 인기... 한강 작가 등 새로운 한류의 시대 크롤링 완료 (댓글 50개)\n",
      "게시글 326164 크롤링 중...\n",
      "[42/50] 교촌치킨보다 더한 100만 베트남 유튜버의 망고 가격 크롤링 완료 (댓글 50개)\n",
      "게시글 326162 크롤링 중...\n",
      "[43/50] 83년생 시누이가 갑자기 유학을 간다고 합니다... 크롤링 완료 (댓글 50개)\n",
      "게시글 326160 크롤링 중...\n",
      "[44/50] 요즘 일본 편의점 도시락 근황 크롤링 완료 (댓글 50개)\n",
      "게시글 326158 크롤링 중...\n",
      "[45/50] SK가 쓰레기 기업인 이유 (feat. 너 잘걸렸다) 크롤링 완료 (댓글 50개)\n",
      "게시글 326154 크롤링 중...\n",
      "[46/50] 꼴릿꼴릿 국가 의인화 크롤링 완료 (댓글 50개)\n",
      "게시글 326152 크롤링 중...\n",
      "[47/50] 일본사람들이 한국사람보다 자전거를 많이 타는 이유 크롤링 완료 (댓글 50개)\n",
      "게시글 326150 크롤링 중...\n",
      "[48/50] SKT해킹과 지역화폐 1조 단독처리 크롤링 완료 (댓글 50개)\n",
      "게시글 326148 크롤링 중...\n",
      "[49/50] 이탈리아인이 말하는 피자 맛있게 먹는 법 크롤링 완료 (댓글 50개)\n",
      "페이지 2 크롤링 중...\n",
      "게시글 326147 크롤링 중...\n",
      "[50/50] 트럼프 파란정장의 진실!! 크롤링 완료 (댓글 50개)\n",
      "\n",
      "✅ CSV 파일 저장 완료: dcinside_silbe_posts.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# 설정\n",
    "BASE_URL = \"https://gall.dcinside.com\"\n",
    "SILBE_URL = f\"{BASE_URL}/board/lists?id=dcbest\"\n",
    "MAX_POSTS = 50\n",
    "MAX_COMMENTS_PER_POST = 50\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\"\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def update_session_headers():\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": random.choice(USER_AGENTS),\n",
    "        \"Referer\": BASE_URL + \"/\"\n",
    "    })\n",
    "\n",
    "update_session_headers()\n",
    "\n",
    "results = []\n",
    "crawled_post_ids = set()\n",
    "\n",
    "# 글 목록에서 링크, post_id, 작성시간, 추천수 추출\n",
    "def get_post_links(page_num):\n",
    "    update_session_headers()\n",
    "    url = f\"{SILBE_URL}&page={page_num}\"\n",
    "    try:\n",
    "        res = session.get(url)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = []\n",
    "        posts = soup.select('tr.ub-content.us-post')\n",
    "        for post in posts:\n",
    "            link_tag = post.select_one('td.gall_tit a')\n",
    "            if link_tag:\n",
    "                href = link_tag.get(\"href\")\n",
    "                if href and '/board/view/' in href:\n",
    "                    full_link = href if href.startswith('http') else BASE_URL + href\n",
    "                    try:\n",
    "                        post_id = href.split('no=')[1].split('&')[0]\n",
    "                    except:\n",
    "                        post_id = \"\"\n",
    "                    try:\n",
    "                        write_time = post.select_one('td.gall_date').text.strip()\n",
    "                    except:\n",
    "                        write_time = \"\"\n",
    "                    try:\n",
    "                        recommend = post.select_one('td.gall_recommend').text.strip()\n",
    "                    except:\n",
    "                        recommend = \"\"\n",
    "                    links.append({\n",
    "                        \"link\": full_link,\n",
    "                        \"post_id\": post_id,\n",
    "                        \"write_time\": write_time,\n",
    "                        \"recommend\": recommend\n",
    "                    })\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"글 목록 가져오기 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "# Selenium으로 댓글 크롤링\n",
    "def get_comments_selenium(post_id):\n",
    "    comments = []\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    try:\n",
    "        post_url = f\"{BASE_URL}/board/view/?id=dcbest&no={post_id}\"\n",
    "        driver.get(post_url)\n",
    "        time.sleep(2)\n",
    "        # 댓글 여러 페이지 지원: 더보기 버튼 클릭 반복\n",
    "        while True:\n",
    "            comment_elements = driver.find_elements(By.CLASS_NAME, \"usertxt\")\n",
    "            for elem in comment_elements:\n",
    "                text = elem.text.strip()\n",
    "                if text and text not in comments:\n",
    "                    comments.append(text)\n",
    "                if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                    break\n",
    "            if len(comments) >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "            try:\n",
    "                more_btn = driver.find_element(By.CSS_SELECTOR, \".comment_more_btn\")\n",
    "                if more_btn.is_displayed():\n",
    "                    more_btn.click()\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Selenium 댓글 수집 오류 (게시글 {post_id}): {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return comments\n",
    "\n",
    "# 글 본문 + 댓글 + 작성시간 + 추천수\n",
    "def get_post_details(post_info):\n",
    "    post_url = post_info['link']\n",
    "    post_id = post_info['post_id']\n",
    "    write_time = post_info['write_time']\n",
    "    recommend = post_info['recommend']\n",
    "    update_session_headers()\n",
    "    res = session.get(post_url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    try:\n",
    "        title = soup.select_one(\"span.title_subject\").text.strip()\n",
    "    except:\n",
    "        title = \"제목 없음\"\n",
    "    try:\n",
    "        content = soup.select_one(\"div.write_div\").text.strip()\n",
    "    except:\n",
    "        content = \"본문 없음\"\n",
    "    comments = get_comments_selenium(post_id)\n",
    "    return {\n",
    "        \"post_id\": post_id,\n",
    "        \"title\": title,\n",
    "        \"link\": post_url,\n",
    "        \"content\": content,\n",
    "        \"write_time\": write_time,\n",
    "        \"recommend\": recommend,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "\n",
    "def crawl_silbe():\n",
    "    page = 1\n",
    "    crawled_posts = 0\n",
    "    while crawled_posts < MAX_POSTS:\n",
    "        print(f\"페이지 {page} 크롤링 중...\")\n",
    "        post_links = get_post_links(page)\n",
    "        if not post_links:\n",
    "            print(f\"페이지 {page}에서 글을 찾을 수 없습니다. 다음 페이지로 넘어갑니다.\")\n",
    "            page += 1\n",
    "            time.sleep(random.uniform(1.0, 2.0))\n",
    "            continue\n",
    "        for post_info in post_links:\n",
    "            if crawled_posts >= MAX_POSTS:\n",
    "                break\n",
    "            post_id = post_info['post_id']\n",
    "            if post_id in crawled_post_ids:\n",
    "                continue\n",
    "            try:\n",
    "                print(f\"게시글 {post_id} 크롤링 중...\")\n",
    "                post = get_post_details(post_info)\n",
    "                results.append(post)\n",
    "                crawled_post_ids.add(post_id)\n",
    "                crawled_posts += 1\n",
    "                print(f\"[{crawled_posts}/{MAX_POSTS}] {post['title']} 크롤링 완료 (댓글 {len(post['comments'])}개)\")\n",
    "                time.sleep(random.uniform(0.8, 1.5))\n",
    "            except Exception as e:\n",
    "                print(f\"게시글 크롤링 오류 (패스): {e}\")\n",
    "                continue\n",
    "        page += 1\n",
    "        time.sleep(random.uniform(1.5, 3.0))\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('dcinside_silbe_posts.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\n✅ CSV 파일 저장 완료: dcinside_silbe_posts.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_silbe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e567db70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 실패: 400 Client Error: Bad Request for url: http://localhost:1234/v1/chat/completions\n",
      "\n",
      "✅ 리포트 작성 완료: community_report.md 파일 생성됨!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import ast\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "import time\n",
    "\n",
    "# nltk 데이터 다운로드 (최초 1번)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. 파일 로드\n",
    "df = pd.read_csv('final_classified_posts.csv')\n",
    "\n",
    "# comments 컬럼 파싱\n",
    "def parse_comments(comment_str):\n",
    "    try:\n",
    "        return ast.literal_eval(comment_str)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df['comments'] = df['comments'].apply(parse_comments)\n",
    "\n",
    "# 2. 주제별 Top 3 글 추리기\n",
    "def get_top3_by_topic(df):\n",
    "    topic_to_posts = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        topics = [t.strip() for t in row['classified_topics'].split(',')]\n",
    "        for topic in topics:\n",
    "            if topic not in topic_to_posts:\n",
    "                topic_to_posts[topic] = []\n",
    "            topic_to_posts[topic].append((row['recommend'], row['title'], row['content']))\n",
    "    \n",
    "    # 추천수 기준 Top 3만\n",
    "    for topic in topic_to_posts:\n",
    "        topic_to_posts[topic] = sorted(topic_to_posts[topic], key=lambda x: x[0], reverse=True)[:3]\n",
    "\n",
    "    return topic_to_posts\n",
    "\n",
    "top3_by_topic = get_top3_by_topic(df)\n",
    "\n",
    "# 3. 핫 키워드 추출\n",
    "okt = Okt()\n",
    "\n",
    "def extract_keywords(texts, top_n=10):\n",
    "    all_nouns = []\n",
    "    for text in texts:\n",
    "        nouns = okt.nouns(text)\n",
    "        all_nouns.extend(nouns)\n",
    "    \n",
    "    counter = Counter(all_nouns)\n",
    "    most_common = counter.most_common(top_n)\n",
    "    return [word for word, _ in most_common]\n",
    "\n",
    "# 모든 글+댓글 합쳐서 키워드 추출\n",
    "all_texts = df['content'].tolist() + [\" \".join(comments) for comments in df['comments'].tolist()]\n",
    "hot_keywords = extract_keywords(all_texts)\n",
    "\n",
    "# 4. 감정 비율 계산\n",
    "sentiment_counts = df['classified_sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "# 5. 전체 민심 종합 요약 (Gemma 호출)\n",
    "API_URL = \"http://localhost:1234/v1/chat/completions\"\n",
    "MODEL_NAME = \"gemma:3-12b-instruct\"\n",
    "\n",
    "SYSTEM_PROMPT_SUMMARY = \"\"\"당신은 글과 댓글을 분석해서 커뮤니티의 전체적인 분위기를 요약하는 전문가입니다.\n",
    "다음 텍스트를 읽고, 현재 커뮤니티의 민심과 주요 분위기를 리포트로 작성해주세요요.\"\"\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def summarize_trend(text):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT_SUMMARY},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"요약 실패: {e}\")\n",
    "        return \"요약 실패\"\n",
    "\n",
    "combined_text_for_summary = \" \".join(df['content'].tolist())\n",
    "overall_summary = summarize_trend(combined_text_for_summary)\n",
    "\n",
    "# 6. Markdown 리포트 생성\n",
    "md = \"# 🔥 오늘의 커뮤니티 민심 리포트\\n\\n\"\n",
    "\n",
    "md += \"## 📰 급상승 키워드 Top 10\\n\"\n",
    "for kw in hot_keywords:\n",
    "    md += f\"- {kw}\\n\"\n",
    "\n",
    "md += \"\\n## 🎯 주제별 인기글 요약\\n\"\n",
    "for topic, posts in top3_by_topic.items():\n",
    "    md += f\"### [{topic}]\\n\"\n",
    "    for _, title, content in posts:\n",
    "        md += f\"- **{title}**: {content[:100]}...\\n\"\n",
    "    md += \"\\n\"\n",
    "\n",
    "md += \"## ❤️ 감정 분석 결과\\n\"\n",
    "for sentiment, percent in sentiment_counts.items():\n",
    "    md += f\"- {sentiment}: {percent:.1f}%\\n\"\n",
    "\n",
    "md += \"\\n## 🧠 전체 민심 요약\\n\"\n",
    "md += f\"> {overall_summary}\\n\"\n",
    "\n",
    "# 7. Markdown 저장\n",
    "with open(\"community_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md)\n",
    "\n",
    "print(\"\\n✅ 리포트 작성 완료: community_report.md 파일 생성됨!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14a9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
